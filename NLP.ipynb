{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Bag of words**"
      ],
      "metadata": {
        "id": "AV0W8e7q2PC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **unigram VS bigram**"
      ],
      "metadata": {
        "id": "leGJ4jIs2XbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "epSFiJtKrYvJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x=[\"i love the book\",\"this book is great\",\"the fit is great\",\"i love the shoes\"]\n",
        "class Category:\n",
        "  BOOKS=\"BOOKS\"\n",
        "  CLOTHING=\"CLOTHING\"\n",
        "\n",
        "train_y=[Category.BOOKS,Category.BOOKS,Category.CLOTHING,Category.CLOTHING]"
      ],
      "metadata": {
        "id": "5uSD2SRgrlQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform the training set to a vector representation\n",
        "les parametres de CountVectorizer:\n",
        "**binary**:on aura les valeurs 0 et 1 pour indiquer si le phrase contient un mot ou non , sinon on aura un nombre qui representera l'occurence de chaque mot\n",
        "**token_pattern**:pour indiquer au vectorizer qu'on veut prendre en conseideration les mots à un seul caractère\n",
        "**ngram_range**:pour faire la tokenerization mot par mot et ensuite par les combianison de deux mots"
      ],
      "metadata": {
        "id": "tyj-Sql1scYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vectorizer=CountVectorizer(binary=True,token_pattern=r'\\b\\w+\\b',ngram_range=(1,2))\n",
        "vectorizer=CountVectorizer(binary=True,token_pattern=r'\\b\\w+\\b')\n",
        "train_x_vectors=vectorizer.fit_transform(train_x)"
      ],
      "metadata": {
        "id": "nD5iHua2sj9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the vector of all the docs/the sentences/the dic"
      ],
      "metadata": {
        "id": "KAzTYXnvtDdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRF3QNQIs5n6",
        "outputId": "93b2aeed-1851-451f-b3e0-c9f4e90ce4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 0)\t1\n",
            "  (1, 0)\t1\n",
            "  (1, 8)\t1\n",
            "  (1, 4)\t1\n",
            "  (1, 2)\t1\n",
            "  (2, 7)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 2)\t1\n",
            "  (2, 1)\t1\n",
            "  (3, 3)\t1\n",
            "  (3, 5)\t1\n",
            "  (3, 7)\t1\n",
            "  (3, 6)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the vector of the first sentence of the dictionnary"
      ],
      "metadata": {
        "id": "csH484iOtK-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x_vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXt3MWiXtPb4",
        "outputId": "98cdec03-e482-4b46-ee08-138466decbbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 3)\t1\n",
            "  (0, 5)\t1\n",
            "  (0, 7)\t1\n",
            "  (0, 0)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vector unique words:\",vectorizer.get_feature_names_out())\n",
        "print(train_x_vectors.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPp1m82rtf1L",
        "outputId": "10373cf2-6d20-4bd5-b660-510c7c08391f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vector unique words: ['book' 'fit' 'great' 'i' 'is' 'love' 'shoes' 'the' 'this']\n",
            "[[1 0 0 1 0 1 0 1 0]\n",
            " [1 0 1 0 1 0 0 0 1]\n",
            " [0 1 1 0 1 0 0 1 0]\n",
            " [0 0 0 1 0 1 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chaque ligne de la matrice represente une phrase du training set . 0 indique que ce mot n'existe pas dans cette phrase et 1 qu'il le contient ( si on ajuste le parametre \"binary\" à True , sinon le nombre sera une representation de l'occurence de chaque mot dans la phrase)"
      ],
      "metadata": {
        "id": "5_embMieum_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "create categories classification"
      ],
      "metadata": {
        "id": "VdowZt40yXZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf_svm=svm.SVC(kernel='linear')\n",
        "clf_svm.fit(train_x_vectors,train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "8HsSLPo0vOUY",
        "outputId": "9b119edd-93cc-4d59-aad3-cae280472a08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(kernel='linear')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing on a new dataset"
      ],
      "metadata": {
        "id": "MbYzBYkCz7UJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_x=vectorizer.transform(['i like the fit','i like the book','shoes are alright'])\n",
        "clf_svm.predict(test_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4iG_C9Cz-cp",
        "outputId": "50cc5c15-7a72-4928-f362-7acd3df4d845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['CLOTHING', 'BOOKS', 'CLOTHING'], dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **word vectors**"
      ],
      "metadata": {
        "id": "FtFTF-S_3Wie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "le probleme de bag of words est que qu'on utilise un mot qui n'appartient pas au training set on aura pas un resultat coherent comme par exemple 'i love books' ou 'i love the story'"
      ],
      "metadata": {
        "id": "ZhTVl-AU3eRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anyDwpk73dlQ",
        "outputId": "cc888b3a-c05d-4350-e044-fd7a5b30acb5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/requirements.py\", line 102, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 1131, in parse_string\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4114, in parseImpl\n",
            "    return e._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3886, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4959, in parseImpl\n",
            "    loc, tokens = self_expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 817, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 3864, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 821, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, pre_loc, doActions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 4375, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pyparsing/core.py\", line 783, in _parseNoCache\n",
            "    if debugging or self.failAction:\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1724, in isEnabledFor\n",
            "    def isEnabledFor(self, level):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\n",
            "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 146, in _get_module_details\n",
            "    return _get_module_details(pkg_main_name, error)\n",
            "  File \"/usr/lib/python3.10/runpy.py\", line 110, in _get_module_details\n",
            "    __import__(pkg_name)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/__init__.py\", line 6, in <module>\n",
            "    from .errors import setup_default_warnings\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/errors.py\", line 3, in <module>\n",
            "    from .compat import Literal\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/spacy/compat.py\", line 4, in <module>\n",
            "    from thinc.util import copy_array\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/__init__.py\", line 5, in <module>\n",
            "    from .config import registry\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/config.py\", line 5, in <module>\n",
            "    from .types import Decorator\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/types.py\", line 25, in <module>\n",
            "    from .compat import cupy, has_cupy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/thinc/compat.py\", line 30, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/__init__.py\", line 1504, in <module>\n",
            "    from . import masked\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/masked/__init__.py\", line 3, in <module>\n",
            "    from ._ops import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/masked/_ops.py\", line 11, in <module>\n",
            "    from torch._prims_common import corresponding_real_dtype\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_prims_common/__init__.py\", line 23, in <module>\n",
            "    import sympy\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/__init__.py\", line 74, in <module>\n",
            "    from .polys import (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/polys/__init__.py\", line 78, in <module>\n",
            "    from .polyfuncs import (symmetrize, horner, interpolate,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/polys/polyfuncs.py\", line 10, in <module>\n",
            "    from sympy.polys.specialpolys import (\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/polys/specialpolys.py\", line 298, in <module>\n",
            "    from sympy.polys.rings import ring\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/polys/rings.py\", line 30, in <module>\n",
            "    from sympy.printing.defaults import DefaultPrinting\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/printing/__init__.py\", line 3, in <module>\n",
            "    from .pretty import pager_print, pretty, pretty_print, pprint, pprint_use_unicode, pprint_try_use_unicode\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sympy/printing/pretty/__init__.py\", line 3, in <module>\n",
            "    from .pretty import (pretty, pretty_print, pprint, pprint_use_unicode,\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "XoQFBhsR8nVY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "s9GPfvMK8uq0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x=[\"i love the book\",\"this book is great\",\"the fit is great\",\"i love the shoes\"]\n",
        "class Category:\n",
        "  BOOKS=\"BOOKS\"\n",
        "  CLOTHING=\"CLOTHING\"\n",
        "\n",
        "train_y=[Category.BOOKS,Category.BOOKS,Category.CLOTHING,Category.CLOTHING]"
      ],
      "metadata": {
        "id": "0vKvXSVY9KpJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs=[nlp(text) for text in train_x ]"
      ],
      "metadata": {
        "id": "b85Dl08K9aRJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTf1Fm3r-M8P",
        "outputId": "16d42093-730b-4921-d4f7-ac9b711f6597"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love the book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqKnnx61BHia",
        "outputId": "60b70716-d299-438f-f0c7-91d0cd8a0d7a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love the book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].vector)"
      ],
      "metadata": {
        "id": "ueY6Pe6--Pj0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f63a5e7a-3768-4752-9fb9-9ee9e80f245f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-3.9804751e-01 -1.7059250e+00 -9.0664995e-01 -4.5425000e+00\n",
            " -1.1165801e+00 -2.9151249e+00  3.1752450e+00  4.0887251e+00\n",
            " -3.4474750e+00  2.3840599e+00  6.4857249e+00  2.3083498e+00\n",
            " -8.6464500e+00  2.0437698e+00  2.2699749e+00 -1.0261000e+00\n",
            "  4.0915399e+00 -7.4801493e-01  1.1435002e-01 -1.9810501e+00\n",
            "  1.3855026e+00  1.7070000e+00 -2.9752648e+00 -1.9328325e+00\n",
            " -1.4255500e+00 -2.0426226e+00 -3.7064652e+00 -4.3784651e-01\n",
            " -2.0860374e+00  4.4308500e+00 -1.0481000e+00 -7.8117514e-01\n",
            " -1.6870000e+00  1.9781501e+00  1.4894226e+00 -2.8325254e-01\n",
            " -1.4800999e+00  1.4303375e+00  2.6068749e+00 -1.3935680e+00\n",
            " -4.5071498e-01  1.8592875e+00  6.0194993e-01 -2.0355899e+00\n",
            "  5.3853750e+00  3.3568425e+00 -2.6558499e+00 -2.5876875e+00\n",
            " -4.1877502e-01  1.1819749e+00 -1.0135007e-01 -1.6391747e+00\n",
            " -7.5115502e-01 -2.4396350e+00 -5.0018353e+00  4.4182479e-02\n",
            " -1.1361099e+00  3.4045234e+00  4.4069252e+00  1.5867125e+00\n",
            "  7.3212752e+00 -6.2377250e-01 -3.5406952e+00 -1.5487249e+00\n",
            " -2.7915027e+00  4.5692497e-01 -2.8675752e+00 -2.1574497e+00\n",
            "  3.5327253e+00  2.9429674e+00 -3.0028498e+00  4.0561576e+00\n",
            " -2.2131751e+00 -2.7301099e+00  2.2525599e+00  6.7574859e-02\n",
            " -2.9405074e+00  1.8029499e+00 -1.1789126e+00 -2.1746502e+00\n",
            " -2.1985504e-01  1.1351575e+00  3.9773331e+00 -1.4294751e+00\n",
            "  2.7083826e-01  2.8169625e+00  3.3689499e+00 -3.6275029e-02\n",
            " -6.6733754e-01 -2.0622499e+00  5.5217499e-01  2.1563876e+00\n",
            "  5.8253503e+00 -5.5494246e+00 -3.5856503e-01 -1.6503401e+00\n",
            " -1.3458250e+00 -1.4840007e-01  1.7622524e+00 -9.1479999e-01\n",
            "  2.1465900e+00  4.4622073e+00  3.2821400e+00  1.8457750e+00\n",
            " -1.8648493e-01  9.6283495e-01 -1.4834499e+00  4.0102500e-01\n",
            " -2.9797997e+00  7.9937494e-01  2.4522400e+00 -1.5166250e+00\n",
            " -3.7715238e-01  2.3985500e+00  3.3255002e-01  1.7986224e+00\n",
            " -6.8490005e-01 -5.1091760e-01 -1.8716326e+00 -8.0151993e-01\n",
            " -1.2196251e+00 -2.4377999e+00 -8.4822536e-01  4.4465375e+00\n",
            "  1.9551001e+00 -4.5647252e-01  1.1451000e+00 -2.7702999e+00\n",
            " -5.0074500e-01 -1.2053250e+00 -5.6294746e+00  1.9835000e+00\n",
            " -2.7191000e+00 -1.5045950e+00  6.9217503e-01  2.5230401e+00\n",
            " -2.1575000e+00 -1.4835875e+00  3.3636250e+00 -1.8662750e+00\n",
            " -2.0677674e+00  1.4370599e+00  1.0521392e+00  2.1033750e+00\n",
            " -2.1464827e+00  5.6450999e-01  5.3212500e-01 -8.3847724e-02\n",
            " -6.6377509e-01  6.8043244e-01 -2.0621850e+00  2.8536501e+00\n",
            " -1.7352247e+00 -2.8443000e+00 -5.3201753e-01  8.3302242e-01\n",
            "  3.7822717e-01  9.0502751e-01 -9.2559004e-01  2.6364100e-01\n",
            " -2.1059022e+00 -4.7094274e+00 -2.3757493e-01  1.3409747e-01\n",
            " -1.4695925e+00  1.1836300e+00 -1.7678249e+00 -2.0474751e+00\n",
            " -3.1015801e+00 -2.7474751e+00  2.3939500e+00  2.9119998e-01\n",
            " -6.4154994e-01  2.9983025e+00  2.3853002e+00 -3.8257499e+00\n",
            "  4.6561250e-01  2.4625850e+00 -4.1938251e-01  6.9829524e-01\n",
            " -7.1579993e-01 -1.4624953e-02  3.2372375e+00 -3.2502999e+00\n",
            "  4.5349836e-02  4.1063027e+00 -2.2001500e+00 -1.6467290e+00\n",
            "  3.1262751e+00  1.4225700e+00  3.6304951e-02  1.4041975e+00\n",
            " -1.5605751e+00  2.3753252e+00  2.6392999e+00 -1.5464250e+00\n",
            " -5.9728498e+00  1.9010249e+00 -1.8664751e+00  5.6060200e+00\n",
            " -9.5827496e-01 -1.9006126e+00 -4.8451276e+00 -5.8231246e-01\n",
            " -7.2317505e-01 -2.2110374e+00  8.1549001e-01 -1.0414025e+00\n",
            "  1.9058951e+00 -3.8605497e+00  3.5286000e+00  1.3662599e+00\n",
            "  2.2550497e+00  2.5063176e+00 -2.7124995e-01  1.9068251e+00\n",
            " -2.3303626e+00 -3.5958242e-01  7.9588258e-01 -1.6820974e+00\n",
            " -3.7828848e+00  2.2385600e+00 -2.3069249e-01  3.8866749e+00\n",
            " -2.9949999e+00 -7.2774887e-02 -5.2936137e-01  2.3862777e+00\n",
            "  2.7844000e+00  2.0868599e+00 -1.0202500e+00 -6.6188755e+00\n",
            "  2.5358500e+00  2.3292500e-01 -1.5609975e+00  1.4913775e+00\n",
            "  4.9717754e-01  6.1425052e+00 -9.3077004e-01 -1.0970299e+00\n",
            " -3.2835951e+00 -4.0366502e+00  2.8092499e+00  5.4461503e-01\n",
            "  5.1136999e+00 -4.7508508e-01 -3.0958500e+00 -1.6869251e+00\n",
            "  5.5714927e+00 -5.8892751e-01 -2.1862252e+00  7.2488189e-04\n",
            " -5.1041250e+00 -2.0981116e+00 -3.4806490e-01 -1.5959325e+00\n",
            " -4.2672509e-01  1.1591926e+00 -3.7716749e+00  2.1917500e+00\n",
            "  2.8775399e+00  3.8451598e+00  4.2834749e+00  2.1570098e+00\n",
            "  2.5055449e+00 -1.2231750e+00  4.1285264e-01  1.1988211e+00\n",
            " -4.9739499e+00  1.0778250e+00  4.2271227e-01 -3.8091974e+00\n",
            " -3.7465799e+00 -1.0162874e+00 -1.5726924e+00 -3.8487256e-01\n",
            "  2.0939450e+00 -3.7680500e+00  2.6195997e-01  2.4638376e+00\n",
            " -1.9473826e+00 -1.5511725e+00  3.0020747e+00  4.0775223e+00\n",
            "  4.7141755e-01 -1.4983499e-01  2.5353000e+00  2.0879149e+00\n",
            " -1.3202243e-01 -2.3845255e-01  4.0010076e+00  1.6600013e-02\n",
            "  4.4546151e+00 -1.3000700e+00 -2.6352043e+00  1.1399500e+00\n",
            "  1.8315576e+00 -2.9688001e+00 -7.6453247e+00  8.4300494e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "VOeSCAsG_czx",
        "outputId": "99c2b5a4-3bf0-4784-ff4f-df3d0be3464e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    127\u001b[0m   )\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([token.text for token in docs[0]])  # Liste des tokens dans la première phrase\n"
      ],
      "metadata": {
        "id": "dcR0EFCECSD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x_word_vectors=[x.vector for x in docs]"
      ],
      "metadata": {
        "id": "RJgUF2QvDEXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create categories"
      ],
      "metadata": {
        "id": "PMAuahZODkyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "clf_svm_wv=svm.SVC(kernel=\"linear\")\n",
        "clf_svm_wv.fit(train_x_word_vectors,train_y)"
      ],
      "metadata": {
        "id": "4wkICwoHDm3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x=[\"i love the books\",\"i love the story\", \"i love the hat\",\"these earings hurt\"]\n",
        "test_docs=[nlp(text) for text in test_x]\n",
        "test_x_word_vectors=[x.vector for x in test_docs]\n",
        "clf_svm_wv.predict(test_x_word_vectors)"
      ],
      "metadata": {
        "id": "NP-8z6MvEU_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Regexes**"
      ],
      "metadata": {
        "id": "PLDBCZ1KJPAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "9LxTErXXJWpa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regex=re.compile(\"^ab[^\\s]*cd$\")"
      ],
      "metadata": {
        "id": "ICP0AyzUNgzR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrases=[\"abcd\",\"xxx\",\"abxxxcd\",\"ab cd\"]"
      ],
      "metadata": {
        "id": "H2SKcKh1Nt1v"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matches=[]\n",
        "for phrase in phrases:\n",
        "  if re.match(regex,phrase):\n",
        "    matches.append(phrase)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDcpb8RsN39C",
        "outputId": "6202aa9e-8920-4016-adb2-a03134af2fbe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abcd', 'abxxxcd']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regex=re.compile(\"read|story|book\")\n",
        "phrases=[\"i liked that story\",\"i like that book\",\"this hat is nice\"]\n",
        "matches=[]\n",
        "for phrase in phrases:\n",
        "  if re.search(regex,phrase):\n",
        "    matches.append(phrase)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0H4pQQUOvVW",
        "outputId": "7584a829-c2b3-4c0f-e387-04c9a01d396a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i liked that story', 'i like that book']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regex=re.compile(\"read|\\bstory\\b|book\")\n",
        "phrases=[\"i liked that history\",\"i like that book\",\"this hat is nice\"]\n",
        "matches=[]\n",
        "for phrase in phrases:\n",
        "  if re.search(regex,phrase):\n",
        "    matches.append(phrase)\n",
        "print(matches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vgvn-86kPoay",
        "outputId": "b2da9f8d-16d0-458b-a3d0-aecad395a33e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i like that book']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **stemming and lemmatization**"
      ],
      "metadata": {
        "id": "cFRmZQxHRKUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **stemming**"
      ],
      "metadata": {
        "id": "eGhS4gMAVlah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nufBrcNcRP4K",
        "outputId": "6063db3f-d7ed-458e-b89f-25a9252e7fff"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "LtRbe49hReMX"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "metadata": {
        "id": "NrW92lKVRv4V"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase=\"reading the stories\"\n",
        "words=word_tokenize(phrase)\n",
        "stemmed_words=[]\n",
        "for word in words:\n",
        "  stemmed_words.append(stemmer.stem(word))\n",
        "\" \".join(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LCWwoqmMUehf",
        "outputId": "afbaeb37-0952-4c22-d4f8-62e2f97a869b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'read the stori'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **lemmatization**"
      ],
      "metadata": {
        "id": "5A9lWLeTVpj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "02zII4NRVs0Q"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ox9vkgekV1jV"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase=\"reading the stories\"\n",
        "words=word_tokenize(phrase)\n",
        "lemmatized_words=[]\n",
        "for word in words:\n",
        "  lemmatized_words.append(lemmatizer.lemmatize(word,pos='v'))\n",
        "\" \".join(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jFFaymuBV7b4",
        "outputId": "49285074-0af6-418b-a48c-724606f5101f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'read the stories'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stop words removal**"
      ],
      "metadata": {
        "id": "V7z5FZD-YZ2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "I0xs-44-Yey8"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words=stopwords.words('english')\n",
        "print(len(stop_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNwN8wnZYqr2",
        "outputId": "514d81d1-4a69-4f2f-eed7-97824d3f34f4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase=\"here is an example sentence demonstrating the removal of stopwords\"\n",
        "words=word_tokenize(phrase)\n",
        "\n",
        "stripped_phrase=[]\n",
        "for word in words:\n",
        "  if word not in stop_words:\n",
        "    stripped_phrase.append(word)\n",
        "print(stripped_phrase)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMTkAkHfZEjb",
        "outputId": "8187752f-9b66-4eb5-cf0a-e2e9c2a8c9bc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrating', 'removal', 'stopwords']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **spell correction/sentiment/pos tagging**"
      ],
      "metadata": {
        "id": "3Vz2peyxazaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "nGDp8tVra3NJ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m textblob.download_corpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgKvcFYbcRwq",
        "outputId": "a1a8ee59-14e5-413b-e40b-e87773ba0b59"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase=\"thiis is an exaazmple\"\n",
        "tb_phrase=TextBlob(phrase)\n",
        "tb_phrase.correct()\n",
        "\n",
        "tb_phrase.tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bduq6umSa7Pi",
        "outputId": "bfcaf7f2-b8f9-4f80-a085-3c7a72557168"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('thiis', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('exaazmple', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "phrase=\"the book sucked, it was so bad\"\n",
        "phrase2=\"the book was horrible\"\n",
        "\n",
        "tb_phrase=TextBlob(phrase)\n",
        "\n",
        "tb_phrase2=TextBlob(phrase2)\n",
        "\n",
        "tb_phrase2.sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n0cxfCTdRRG",
        "outputId": "02d9187d-db3b-4eff-dd8a-f8576b119d0d"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=-1.0, subjectivity=1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    }
  ]
}